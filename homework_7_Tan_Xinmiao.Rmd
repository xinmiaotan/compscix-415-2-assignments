---
title: "homework_7_Tan_Xinmiao"
author: "Xinmiao Tan"
date: "July 24, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)
```

### Exercise 1
#### Load the train.csv dataset into R. How many observations and columns are there?

```{r}
file_path <- '/Users/xinmiaotan/Desktop/Data Science/R/IntroR/Lec7/train.csv'
train <- read_csv(file = file_path)
glimpse(train)
```

Observations: 1,460
Columns Variables: 81


### Exercise 2

#### Normally at this point you would spend a few days on EDA, but for this homework we will do some very basic EDA and get right to fitting some linear regression models.
#### Our target will be SalePrice.

#### • Visualize the distribution of SalePrice.

```{r}
ggplot(data=train,mapping=aes(x=SalePrice))+
    geom_histogram()
```

#### • Visualize the covariation between SalePrice and Neighborhood. 

```{r}
ggplot(data=train,mapping=aes(x = reorder(Neighborhood, SalePrice, median), y=SalePrice))+
    geom_boxplot()+
    xlab('Neighborhood') +
    ylab('MEDIAN SalePrice') +
    coord_flip()
```

#### • Visualize the covariation between SalePrice and OverallQual.

```{r}
ggplot(data=train, mapping=aes(x=OverallQual, y=SalePrice))+
    geom_smooth()
```


### Exercise 3
#### Our target is called SalePrice. First, we can fit a simple regression model consisting of only the intercept (the average of SalePrice). Fit the model and then use the broom package to

#### • take a look at the coefficient

```{r}
train_lm1 <- lm(formula = SalePrice ~ 1, data = train)
tidy(train_lm1)
```

The coefficient is 180921.2.

#### • compare the coefficient to the average value of SalePrice, and 

```{r}
mean(train$SalePrice)
```

The coefficient is 180921.2, which is the same as the mean of SalePrice (180921.2)

#### • take a look at the R-squared.

```{r}
glance(train_lm1)
```

R-squared is 0.

### Exercise 4

#### Now fit a linear regression model using GrLivArea, OverallQual, and Neighborhood as the features. Don’t forget to look at data_description.txt to understand what these variables mean. Ask yourself these questions before fitting the model:

#### • What kind of relationship will these features have with our target? 
GrLivArea: the larger the GrLivArea, the higher the sales price.
OverallQual: the larger the OverallQual, the higher the sales price.
Neighborhood: different Neighborhoods should have very different sales price.

#### • Can the relationship be estimated linearly?

It can be estimated linearly. Other non-linearly models can be tested also. Use the better fitted models.

#### • Are these good features, given the problem we are trying to solve?

These are good features, because they impact the sales price logically.

#### After fitting the model, output the coefficients and the R-squared using the broom package. Answer these questions:

```{r}
train_lm2 <- lm(formula = SalePrice ~ GrLivArea + OverallQual + Neighborhood, data = train)
tidy(train_lm2)
glance(train_lm2)
```

#### • How would you interpret the coefficients on GrLivArea and OverallQual? 

For every 1 unit increase in GrLivArea, sales price increase 55.5645.
For every 1 unit increase in OverallQual, sales price increase 20951.4249.

#### • How would you interpret the coefficient on NeighborhoodBrkSide?

When other features (GrLivArea and OverallQual) with same value, house with NeighborhoodBrkSide on average have 13025.4529 less sales price comparing the sales price of NeighborhoodBlmngtn

#### • Are the features significant?

All three features are significant.

#### • Are the features practically significant?

GrLivArea and OverallQual is practically significant. Some groups of Neighborhood is practically significant.

#### • Is the model a good fit?

R square = 0.7868484. AIC =34887.25. The model is a good fit.


### Exercise 5 (OPTIONAL - won’t be graded)

#### Feel free to play around with linear regression. Add some other features and see how the model results change.

### Exercise 6

#### One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below (use y as the target and x as the feature), and look at the resulting coefficients and R-squared. Rerun it about 5-6 times to generate different simulated datasets. What do you notice about the model’s coefficient on x and the R-squared values?


```{r}
n <- 6 #how many samples
slopes <- rep(NA, n) # empty vector for saving coefficients
r <- rep(NA, n) # empty vector for saving r
for(i in 1:n) {
  sim1a <- tibble(
                x = rep(1:10, each = 3),
                y = x * 1.5 + 6 + rt(length(x), df = 2)
                ) 
  sim1a_lm <- lm(y ~ x, data = sim1a)
  slopes[i] <- coef(sim1a_lm)[2] # store the coefficient
  r[i] <- as.numeric(glance(sim1a_lm)[1])
}
slopes <- slopes
r <- r
slopes
r
mean(slopes)
mean(r)
```

The coefficient of X is around 1.5, and the R square is very high (85%)

